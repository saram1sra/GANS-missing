{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries import \n",
    "import tensorflow as tf\n",
    "#from tensorflow_federated import python as tff\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.datasets import mnist,cifar10\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Normalize Data \n",
    "# 1. MNIST/EMNIST Data - handwritten things \n",
    "from emnist import extract_training_samples\n",
    "def normalize(data, factor): \n",
    "    return ((data-factor)/factor)\n",
    "def load_handwritten_data(digits_only=True):\n",
    "    # load data\n",
    "    if(digits_only): \n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    else: \n",
    "        x_, y_ = extract_training_samples('letters')\n",
    "        test_ind = np.random.randint(0, x_.shape[0], int(1/7*x_.shape[0]))\n",
    "        x_test, y_test = x_[test_ind,...], y_[test_ind,...]\n",
    "        x_train, y_train = np.delete(x_, test_ind, axis=0), np.delete(y_, test_ind,axis=0)\n",
    "    \n",
    "    # normalize data \n",
    "    normal_train_factor, normal_test_factor = np.max(x_train.flatten())/2, np.max(x_test.flatten())/2\n",
    "    x_train, x_test = normalize(x_train, normal_train_factor), normalize(x_test, normal_test_factor)\n",
    "    return(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# 2. CIFAR10 Dataset - \n",
    "def load_CIFAR_data():\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    return(x_train, y_train, x_test, y_test)\n",
    "\n",
    "\n",
    "# Visualizing the images \n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "def plot_imgs(imgs, plt_per_lines=4): \n",
    "    %matplotlib inline\n",
    "    if(type(imgs) != list): \n",
    "        print(\"Please pass list of images\")\n",
    "        return\n",
    "    n = len(imgs)\n",
    "    fig, axs = plt.subplots(m.ceil(n/plt_per_lines),plt_per_lines)\n",
    "    for i,ax in enumerate(axs.flat):\n",
    "        if( i >= n): return\n",
    "        ax.imshow(imgs[i], cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "# train, y_train, test, y_test  = load_handwritten_data()\n",
    "train, y_train, test, y_test  = load_CIFAR_data()\n",
    "#use LUMA coding to turn to grayscale \n",
    "\n",
    "plot_imgs([i for i in train[0:9, ...]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic descriminator \n",
    "def discriminator(n_classes, image_shape): \n",
    "#    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "#    with mirrored_strategy.scope():\n",
    "    dmod = Sequential()\n",
    "    dmod.add(Dense(512, activation ='sigmoid', input_shape=(image_shape,)))\n",
    "    dmod.add(LeakyReLU(0.2))\n",
    "    dmod.add(Dropout(1-0.3)) #reduces overfitting \n",
    "    dmod.add(Dense(n_classes, activation='softmax'))\n",
    "    dmod.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[tf.keras.metrics.categorical_accuracy, 'accuracy'])\n",
    "                  #,distribute=mirrored_strategy)\n",
    "    return dmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example descriminator - implemented here https://www.datacamp.com/community/tutorials/generative-adversarial-networks\n",
    "# discriminator = Sequential()\n",
    "# discriminator.add(Dense(1072, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "# discriminator.add(LeakyReLU(0.2))\n",
    "# discriminator.add(Dropout(1-0.3))\n",
    "\n",
    "# discriminator.add(Dense(512))\n",
    "# discriminator.add(LeakyReLU(0.2))\n",
    "# discriminator.add(Dropout(1-0.3))\n",
    "\n",
    "# discriminator.add(Dense(256))\n",
    "# discriminator.add(LeakyReLU(0.2))\n",
    "# discriminator.add(Dropout(1-0.3))\n",
    "\n",
    "# discriminator.add(Dense(1, activation='sigmoid'))\n",
    "# discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5),  metrics=[tf.keras.metrics.categorical_accuracy, 'accuracy'])\n",
    "# discriminator.evaluate(test.reshape(test.shape[0], test.shape[1]*test.shape[2]), y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on MNIST\n",
    "train, y_train, test, y_test  = load_handwritten_data()\n",
    "y_ = np.eye(np.max(y_train)+1)[y_train.reshape(-1)]\n",
    "image_shape = np.prod(train.shape[1:])\n",
    "dmod = discriminator(np.max(y_train)+1, image_shape)\n",
    "print(\"loaded_discriminator\")\n",
    "print(\"start testing\")\n",
    "dmod.fit(train.reshape(train.shape[0], -1), y_, epochs=10, batch_size=32 )\n",
    "\n",
    "# Test on MNIST\n",
    "y_t = np.eye(np.max(y_test)+1)[y_test.reshape(-1)]\n",
    "dmod.evaluate(test.reshape(test.shape[0], -1), y_t, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on EMNIST\n",
    "etrain, eytrain, etest, eytest  = load_handwritten_data(False)\n",
    "ey_  = np.eye(np.max(eytrain)+1)[eytrain.reshape(-1)]\n",
    "image_shape = np.prod(etrain.shape[1:])\n",
    "dmod = discriminator(np.max(eytrain)+1, image_shape)\n",
    "dmod.fit(etrain.reshape(etrain.shape[0],-1), ey_, batch_size=32, epochs=10)\n",
    "\n",
    "# Test on EMNIST\n",
    "ey_t = np.eye(np.max(eytest)+1)[eytest.reshape(-1)]\n",
    "dmod.evaluate(etest.reshape(etest.shape[0], -1), ey_t, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on CIFAR\n",
    "train, y_train, test, y_test  = load_CIFAR_data()\n",
    "y_ = np.eye(np.max(y_train)+1)[y_train.reshape(-1)]\n",
    "image_shape = np.prod(train.shape[1:])\n",
    "dmod = discriminator(np.max(y_train)+1, image_shape)\n",
    "print(\"loaded_discriminator\")\n",
    "print(\"start testing\")\n",
    "dmod.fit(train.reshape(train.shape[0], -1), y_, epochs=10, batch_size=32 )\n",
    "\n",
    "# Test on CIFAR\n",
    "y_t = np.eye(np.max(y_test)+1)[y_test.reshape(-1)]\n",
    "dmod.evaluate(test.reshape(test.shape[0], -1), y_t, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing a generator \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (tf-env)",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
